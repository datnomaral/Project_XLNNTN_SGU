{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ĐỒ ÁN XỬ LÝ NGÔN NGỮ TỰ NHIÊN\n",
    "## Dịch máy Anh-Pháp với mô hình Encoder-Decoder LSTM\n",
    "\n",
    "**HK1 / 2025-2026**\n",
    "\n",
    "---\n",
    "\n",
    "### Mục tiêu:\n",
    "- Xây dựng mô hình **Encoder-Decoder LSTM** từ đầu\n",
    "- Sử dụng **context vector cố định** (không attention)\n",
    "- Dịch từ **tiếng Anh sang tiếng Pháp**\n",
    "- Dataset: **Multi30K (en-fr)**\n",
    "- Đánh giá: **BLEU score**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cài đặt thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cài đặt các thư viện cần thiết\n",
    "!pip install torch torchtext spacy nltk matplotlib seaborn tqdm -q\n",
    "\n",
    "# Download spaCy models\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "!python -m spacy download fr_core_news_sm -q\n",
    "\n",
    "print(\"✓ Đã cài đặt xong các thư viện!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## 2. Import thư viện và modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Import thành công!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Import các modules đã tạo\n",
    "import sys\n",
    "sys.path.append('src')\n",
    "\n",
    "from src.data_utils import get_tokenizers, Vocabulary, build_vocab_from_iterator, get_data_loaders\n",
    "from src.model import Encoder, Decoder, Seq2Seq\n",
    "from src.train import train, count_parameters, initialize_weights, load_checkpoint\n",
    "from src.evaluate import calculate_bleu_score, plot_training_history, plot_bleu_scores, analyze_translation_errors, save_error_analysis\n",
    "from src.translate import translate, greedy_decode, beam_search_decode, interactive_translation\n",
    "\n",
    "print(\"✓ Import thành công!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Thiết lập seed và device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Set seed cho reproducibility\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tải và chuẩn bị dữ liệu\n",
    "### 4.1. Download dataset Multi30K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Could not load this library: D:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torchtext\\lib\\libtorchtext.pyd",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torch\\_ops.py:1488\u001b[39m, in \u001b[36m_Ops.load_library\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m     \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1489\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ctypes\\__init__.py:376\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mOSError\u001b[39m: [WinError 127] The specified procedure could not be found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Multi30k\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDownloading Multi30K dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Download dataset\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Note: Multi30K có thể cần download thủ công nếu link không khả dụng\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torchtext\\__init__.py:18\u001b[39m\n\u001b[32m     15\u001b[39m     _WARN = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     20\u001b[39m _TEXT_BUCKET = \u001b[33m\"\u001b[39m\u001b[33mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m _CACHE_DIR = os.path.expanduser(os.path.join(_get_torch_home(), \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torchtext\\_extension.py:64\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torchtext\\_extension.py:58\u001b[39m, in \u001b[36m_init_extension\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils.is_module_available(\u001b[33m\"\u001b[39m\u001b[33mtorchtext._torchtext\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mtorchtext C++ Extension is not found.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlibtorchtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torchtext\\_extension.py:50\u001b[39m, in \u001b[36m_load_lib\u001b[39m\u001b[34m(lib)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torch\\_ops.py:1490\u001b[39m, in \u001b[36m_Ops.load_library\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m   1488\u001b[39m         ctypes.CDLL(path)\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1490\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not load this library: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1491\u001b[39m \u001b[38;5;28mself\u001b[39m.loaded_libraries.add(path)\n",
      "\u001b[31mOSError\u001b[39m: Could not load this library: D:\\ĐỒ ÁN XỬ LÍ NGÔN NGỮ TỰ NHIÊN\\venv\\Lib\\site-packages\\torchtext\\lib\\libtorchtext.pyd"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "print(\"Downloading Multi30K dataset...\")\n",
    "\n",
    "# Download dataset\n",
    "# Note: Multi30K có thể cần download thủ công nếu link không khả dụng\n",
    "try:\n",
    "    train_data, val_data, test_data = Multi30k(split=('train', 'valid', 'test'), language_pair=('en', 'fr'))\n",
    "    \n",
    "    # Convert to list\n",
    "    train_data = list(train_data)\n",
    "    val_data = list(val_data)\n",
    "    test_data = list(test_data)\n",
    "    \n",
    "    print(f\"✓ Train: {len(train_data)} samples\")\n",
    "    print(f\"✓ Validation: {len(val_data)} samples\")\n",
    "    print(f\"✓ Test: {len(test_data)} samples\")\n",
    "    \n",
    "    # Hiển thị ví dụ\n",
    "    print(\"\\nVí dụ:\")\n",
    "    en, fr = train_data[0]\n",
    "    print(f\"English: {en}\")\n",
    "    print(f\"French:  {fr}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Lỗi khi download: {e}\")\n",
    "    print(\"\\nHướng dẫn download thủ công:\")\n",
    "    print(\"1. Truy cập: https://github.com/multi30k/dataset\")\n",
    "    print(\"2. Download các file: train.en, train.fr, val.en, val.fr, test.en, test.fr\")\n",
    "    print(\"3. Đặt vào thư mục data/multi30k/\")\n",
    "    print(\"4. Uncomment đoạn code dưới để load từ file local\")\n",
    "    \n",
    "    # # BACKUP: Load từ file local (nếu download thủ công)\n",
    "    # def load_data_from_files(base_path='data/multi30k'):\n",
    "    #     def read_file(path):\n",
    "    #         with open(path, 'r', encoding='utf-8') as f:\n",
    "    #             return [line.strip() for line in f]\n",
    "    #     \n",
    "    #     train_en = read_file(f'{base_path}/train.en')\n",
    "    #     train_fr = read_file(f'{base_path}/train.fr')\n",
    "    #     val_en = read_file(f'{base_path}/val.en')\n",
    "    #     val_fr = read_file(f'{base_path}/val.fr')\n",
    "    #     test_en = read_file(f'{base_path}/test.en')\n",
    "    #     test_fr = read_file(f'{base_path}/test.fr')\n",
    "    #     \n",
    "    #     return (list(zip(train_en, train_fr)), \n",
    "    #             list(zip(val_en, val_fr)), \n",
    "    #             list(zip(test_en, test_fr)))\n",
    "    # \n",
    "    # train_data, val_data, test_data = load_data_from_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Khởi tạo tokenizers...\")\n",
    "tokenize_en, tokenize_fr = get_tokenizers()\n",
    "\n",
    "# Test tokenizer\n",
    "test_en = \"A man sitting on a bench.\"\n",
    "test_fr = \"Un homme assis sur un banc.\"\n",
    "\n",
    "print(f\"\\nEnglish: {test_en}\")\n",
    "print(f\"Tokens:  {tokenize_en(test_en)}\")\n",
    "print(f\"\\nFrench: {test_fr}\")\n",
    "print(f\"Tokens: {tokenize_fr(test_fr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Xây dựng từ điển (Vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Xây dựng vocabulary...\")\n",
    "\n",
    "# Tách source và target\n",
    "train_en = [pair[0] for pair in train_data]\n",
    "train_fr = [pair[1] for pair in train_data]\n",
    "\n",
    "# Build vocabularies (giới hạn 10,000 từ theo đề)\n",
    "en_vocab = build_vocab_from_iterator(train_en, tokenize_en, max_size=10000)\n",
    "fr_vocab = build_vocab_from_iterator(train_fr, tokenize_fr, max_size=10000)\n",
    "\n",
    "print(f\"\\n✓ English vocabulary size: {len(en_vocab)}\")\n",
    "print(f\"✓ French vocabulary size: {len(fr_vocab)}\")\n",
    "\n",
    "# Test numericalization\n",
    "tokens = tokenize_en(\"hello world\")\n",
    "indices = en_vocab.numericalize(tokens)\n",
    "print(f\"\\nTest: {tokens} -> {indices}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Tạo DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 64  # Có thể điều chỉnh: 32-128\n",
    "\n",
    "print(f\"Tạo DataLoaders với batch size = {BATCH_SIZE}...\")\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders(\n",
    "    train_data, val_data, test_data,\n",
    "    en_vocab, fr_vocab,\n",
    "    tokenize_en, tokenize_fr,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0  # Tăng nếu có nhiều CPU cores\n",
    ")\n",
    "\n",
    "print(f\"✓ Train batches: {len(train_loader)}\")\n",
    "print(f\"✓ Val batches: {len(val_loader)}\")\n",
    "print(f\"✓ Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test một batch\n",
    "src, src_lengths, tgt = next(iter(train_loader))\n",
    "print(f\"\\nBatch shapes:\")\n",
    "print(f\"  Source: {src.shape}       (batch_size, max_src_len)\")\n",
    "print(f\"  Lengths: {src_lengths.shape}  (batch_size,)\")\n",
    "print(f\"  Target: {tgt.shape}       (batch_size, max_tgt_len)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Xây dựng mô hình\n",
    "### 5.1. Định nghĩa hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters (theo đề tài)\n",
    "INPUT_DIM = len(en_vocab)      # Kích thước vocab tiếng Anh\n",
    "OUTPUT_DIM = len(fr_vocab)     # Kích thước vocab tiếng Pháp\n",
    "ENC_EMB_DIM = 256              # Embedding dimension for encoder (256-512)\n",
    "DEC_EMB_DIM = 256              # Embedding dimension for decoder\n",
    "HIDDEN_SIZE = 512              # Hidden size (512)\n",
    "NUM_LAYERS = 2                 # Số layer LSTM (2)\n",
    "DROPOUT = 0.5                  # Dropout rate (0.3-0.5)\n",
    "\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  Input vocabulary:  {INPUT_DIM}\")\n",
    "print(f\"  Output vocabulary: {OUTPUT_DIM}\")\n",
    "print(f\"  Embedding dim:     {ENC_EMB_DIM}\")\n",
    "print(f\"  Hidden size:       {HIDDEN_SIZE}\")\n",
    "print(f\"  Num layers:        {NUM_LAYERS}\")\n",
    "print(f\"  Dropout:           {DROPOUT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Khởi tạo Encoder, Decoder, Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Khởi tạo Encoder\n",
    "encoder = Encoder(\n",
    "    input_size=INPUT_DIM,\n",
    "    embedding_dim=ENC_EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Khởi tạo Decoder\n",
    "decoder = Decoder(\n",
    "    output_size=OUTPUT_DIM,\n",
    "    embedding_dim=DEC_EMB_DIM,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Khởi tạo Seq2Seq\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Khởi tạo weights\n",
    "initialize_weights(model)\n",
    "\n",
    "print(\"✓ Model đã được khởi tạo!\")\n",
    "print(f\"✓ Tổng số parameters: {count_parameters(model):,}\")\n",
    "\n",
    "# Hiển thị kiến trúc\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Huấn luyện mô hình\n",
    "### 6.1. Định nghĩa optimizer, loss, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "LEARNING_RATE = 0.001          # Learning rate (theo đề: Adam lr=0.001)\n",
    "NUM_EPOCHS = 20                # Số epochs tối đa (10-20)\n",
    "CLIP = 1.0                     # Gradient clipping\n",
    "TEACHER_FORCING_RATIO = 0.5    # Teacher forcing ratio (0.5)\n",
    "PATIENCE = 3                   # Early stopping patience\n",
    "\n",
    "# Optimizer: Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss function: CrossEntropyLoss (ignore padding)\n",
    "PAD_IDX = fr_vocab.stoi[fr_vocab.pad_token]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Learning rate scheduler: ReduceLROnPlateau\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Optimizer:             Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"  Loss:                  CrossEntropyLoss\")\n",
    "print(f\"  Scheduler:             ReduceLROnPlateau\")\n",
    "print(f\"  Epochs:                {NUM_EPOCHS}\")\n",
    "print(f\"  Gradient clipping:     {CLIP}\")\n",
    "print(f\"  Teacher forcing ratio: {TEACHER_FORCING_RATIO}\")\n",
    "print(f\"  Early stopping:        {PATIENCE} epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Bắt đầu training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    clip=CLIP,\n",
    "    teacher_forcing_ratio=TEACHER_FORCING_RATIO,\n",
    "    device=device,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    patience=PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Visualize training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history, save_path='results/training_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best checkpoint\n",
    "model, checkpoint = load_checkpoint(model, 'checkpoints/best_model.pth', device)\n",
    "\n",
    "print(\"\\n✓ Best model đã được load!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Đánh giá mô hình\n",
    "### 8.1. Tính BLEU score trên test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import calculate_bleu_score\n",
    "from src.translate import translate_sentence\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "# Download nltk data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Đang tính BLEU score trên test set...\")\n",
    "print(\"(Quá trình này có thể mất vài phút)\\n\")\n",
    "\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "# Giới hạn số lượng để test nhanh (hoặc dùng toàn bộ test_data)\n",
    "num_samples = min(1000, len(test_data))  # Tăng lên max để có kết quả chính xác\n",
    "\n",
    "for i, (src, tgt) in enumerate(test_data[:num_samples]):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Progress: {i}/{num_samples}\")\n",
    "    \n",
    "    # Dịch câu\n",
    "    translated = translate_sentence(\n",
    "        model, src, en_vocab, fr_vocab, tokenize_en, device, max_len=50\n",
    "    )\n",
    "    \n",
    "    # Tokenize reference\n",
    "    ref_tokens = tokenize_fr(tgt)\n",
    "    \n",
    "    # Tokenize hypothesis\n",
    "    hyp_tokens = translated.split()\n",
    "    \n",
    "    references.append(ref_tokens)\n",
    "    hypotheses.append(hyp_tokens)\n",
    "\n",
    "# Tính BLEU scores\n",
    "bleu_scores = calculate_bleu_score(references, hypotheses, max_n=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BLEU SCORES ON TEST SET\")\n",
    "print(\"=\"*80)\n",
    "for metric, score in bleu_scores.items():\n",
    "    print(f\"{metric}: {score:.2f}%\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Lưu kết quả\n",
    "import os\n",
    "os.makedirs('results', exist_ok=True)\n",
    "with open('results/bleu_scores.json', 'w') as f:\n",
    "    json.dump(bleu_scores, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ BLEU scores đã lưu tại: results/bleu_scores.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Visualize BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot BLEU scores\n",
    "plot_bleu_scores(bleu_scores, save_path='results/bleu_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Phân tích lỗi dịch thuật"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Phân tích lỗi dịch thuật...\\n\")\n",
    "\n",
    "# Phân tích 5 ví dụ lỗi\n",
    "error_examples = analyze_translation_errors(\n",
    "    model=model,\n",
    "    test_data=test_data,\n",
    "    src_vocab=en_vocab,\n",
    "    tgt_vocab=fr_vocab,\n",
    "    src_tokenizer=tokenize_en,\n",
    "    device=device,\n",
    "    num_examples=5\n",
    ")\n",
    "\n",
    "# Lưu kết quả\n",
    "save_error_analysis(error_examples, save_path='results/error_analysis.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test hàm translate() với câu mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test với một số câu mới\n",
    "test_sentences = [\n",
    "    \"A man is sitting on a bench.\",\n",
    "    \"The cat is sleeping on the sofa.\",\n",
    "    \"Children are playing in the garden.\",\n",
    "    \"She is reading a book.\",\n",
    "    \"The weather is beautiful today.\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST DỊCH CÂU MỚI\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    # Greedy decoding\n",
    "    greedy_trans = translate(\n",
    "        sentence, model, en_vocab, fr_vocab, tokenize_en, device, method='greedy'\n",
    "    )\n",
    "    \n",
    "    # Beam search\n",
    "    beam_trans = translate(\n",
    "        sentence, model, en_vocab, fr_vocab, tokenize_en, device, method='beam', beam_size=3\n",
    "    )\n",
    "    \n",
    "    print(f\"English:        {sentence}\")\n",
    "    print(f\"French (Greedy): {greedy_trans}\")\n",
    "    print(f\"French (Beam):   {beam_trans}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n✓ Hàm translate() hoạt động tốt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Chế độ dịch tương tác (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment để chạy chế độ interactive\n",
    "# interactive_translation(model, en_vocab, fr_vocab, tokenize_en, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Tổng kết\n",
    "### 12.1. Kết quả đạt được"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TỔNG KẾT DỰ ÁN\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. MODEL ARCHITECTURE:\")\n",
    "print(f\"   - Encoder-Decoder LSTM với context vector cố định\")\n",
    "print(f\"   - Hidden size: {HIDDEN_SIZE}\")\n",
    "print(f\"   - Num layers: {NUM_LAYERS}\")\n",
    "print(f\"   - Dropout: {DROPOUT}\")\n",
    "print(f\"   - Total parameters: {count_parameters(model):,}\")\n",
    "\n",
    "print(\"\\n2. DATASET:\")\n",
    "print(f\"   - Multi30K (en-fr)\")\n",
    "print(f\"   - Train: {len(train_data)} samples\")\n",
    "print(f\"   - Validation: {len(val_data)} samples\")\n",
    "print(f\"   - Test: {len(test_data)} samples\")\n",
    "\n",
    "print(\"\\n3. TRAINING:\")\n",
    "print(f\"   - Best validation loss: {checkpoint['best_val_loss']:.4f}\")\n",
    "print(f\"   - Training epochs: {checkpoint['epoch'] + 1}\")\n",
    "print(f\"   - Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print(f\"   - Early stopping: Yes (patience={PATIENCE})\")\n",
    "\n",
    "print(\"\\n4. BLEU SCORES:\")\n",
    "for metric, score in bleu_scores.items():\n",
    "    print(f\"   - {metric}: {score:.2f}%\")\n",
    "\n",
    "print(\"\\n5. FILES GENERATED:\")\n",
    "print(f\"   ✓ checkpoints/best_model.pth\")\n",
    "print(f\"   ✓ results/training_history.json\")\n",
    "print(f\"   ✓ results/training_history.png\")\n",
    "print(f\"   ✓ results/bleu_scores.json\")\n",
    "print(f\"   ✓ results/bleu_scores.png\")\n",
    "print(f\"   ✓ results/error_analysis.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ ĐỒ ÁN HOÀN THÀNH!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTiếp theo:\")\n",
    "print(\"  1. Viết báo cáo PDF (bao gồm sơ đồ kiến trúc, biểu đồ, phân tích lỗi)\")\n",
    "print(\"  2. Đề xuất cải tiến (attention, beam search, augmentation, ...)\")\n",
    "print(\"  3. Nộp: main.ipynb + best_model.pth + report.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Đề xuất cải tiến (Bonus: +1.0 điểm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Các cải tiến có thể thực hiện:\n",
    "\n",
    "1. **Thay greedy decoding bằng beam search** (beam size = 3-5)  \n",
    "   ✅ Đã implement trong `src/translate.py`\n",
    "\n",
    "2. **Thêm Attention Mechanism** (Bahdanau/Luong attention)  \n",
    "   - Context vector động thay vì cố định\n",
    "   - Cải thiện BLEU score đáng kể\n",
    "\n",
    "3. **Sử dụng dataset WMT 2014** (~36 triệu câu thay vì 29k)  \n",
    "   - Tăng kích thước vocab\n",
    "   - Model học được nhiều pattern hơn\n",
    "\n",
    "4. **Tăng số layer LSTM** hoặc **hidden size**  \n",
    "   - 3-4 layers, hidden size 1024\n",
    "   - Cần GPU mạnh hơn\n",
    "\n",
    "5. **Subword tokenization** (BPE - Byte Pair Encoding)  \n",
    "   ✅ Đề tài yêu cầu có thể thử\n",
    "   - Giảm OOV (out-of-vocabulary)\n",
    "   - Sử dụng thư viện `sentencepiece` hoặc `subword-nmt`\n",
    "\n",
    "6. **So sánh hiệu suất với mô hình Transformer**  \n",
    "   - Baseline: LSTM seq2seq\n",
    "   - Advanced: Transformer (self-attention)\n",
    "\n",
    "7. **Overfitting handling**  \n",
    "   - Dropout, layer normalization\n",
    "   - Data augmentation (back-translation)\n",
    "\n",
    "8. **Scheduled sampling** thay vì teacher forcing cố định  \n",
    "   - Giảm dần teacher forcing ratio theo epoch\n",
    "\n",
    "---\n",
    "\n",
    "**Lựa chọn khuyến nghị cho Bonus:**\n",
    "- Thêm **Attention** (dễ implement, cải thiện rõ rệt)\n",
    "- Thử **Beam search** với beam size khác nhau (3, 5, 10)\n",
    "- Phân tích sâu hơn các lỗi dịch (ngữ pháp, ngữ nghĩa)\n",
    "- So sánh với **WMT 2014** dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**HẾT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python NLP (venv)",
   "language": "python",
   "name": "mt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

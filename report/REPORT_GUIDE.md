# H∆∞·ªõng d·∫´n vi·∫øt b√°o c√°o PDF - ƒê·ªì √°n D·ªãch m√°y Anh-Ph√°p

## C·∫•u tr√∫c b√°o c√°o (6-10 trang A4)

### 1. Trang b√¨a
- T√™n tr∆∞·ªùng, khoa
- ƒê·ªÅ t√†i: D·ªäCH M√ÅY ANH-PH√ÅP V·ªöI M√î H√åNH ENCODER-DECODER LSTM
- H·ªç t√™n sinh vi√™n, MSSV
- Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n
- H·ªçc k·ª≥: HK1 / 2025-2026
- Ng√†y n·ªôp: 14/12/2025

---

### 2. M·ª•c l·ª•c

---

### 3. Gi·ªõi thi·ªáu (0.5 trang)
- B·ªëi c·∫£nh: D·ªãch m√°y l√† g√¨? T·∫ßm quan tr·ªçng?
- M·ª•c ti√™u: X√¢y d·ª±ng m√¥ h√¨nh Encoder-Decoder LSTM ƒë·ªÉ d·ªãch Anh-Ph√°p
- Ph·∫°m vi: Dataset Multi30K, context vector c·ªë ƒë·ªãnh
- T·ªï ch·ª©c b√°o c√°o

---

### 4. C∆° s·ªü l√Ω thuy·∫øt (1.5 trang)

#### 4.1. Sequence-to-Sequence Learning
- Gi·ªõi thi·ªáu Seq2Seq (Sutskever et al., 2014)
- ·ª®ng d·ª•ng: d·ªãch m√°y, chatbot, t√≥m t·∫Øt vƒÉn b·∫£n

#### 4.2. LSTM (Long Short-Term Memory)
- V·∫•n ƒë·ªÅ c·ªßa RNN: vanishing/exploding gradients
- C·∫•u tr√∫c LSTM: forget gate, input gate, output gate
- C√¥ng th·ª©c to√°n h·ªçc

#### 4.3. Encoder-Decoder Architecture
- **Encoder**: ƒê·ªçc c√¢u ngu·ªìn ‚Üí context vector (h_n, c_n)
- **Decoder**: Nh·∫≠n context vector ‚Üí sinh c√¢u ƒë√≠ch
- **Context vector c·ªë ƒë·ªãnh**: Kh√¥ng d√πng attention (baseline)

**S∆° ƒë·ªì ki·∫øn tr√∫c:**
```
Input (English)
     ‚Üì
  Embedding
     ‚Üì
 Encoder LSTM (2 layers, hidden=512)
     ‚Üì
Context Vector (h_n, c_n)
     ‚Üì
 Decoder LSTM (2 layers, hidden=512)
     ‚Üì
  Softmax
     ‚Üì
Output (French)
```

---

### 5. D·ªØ li·ªáu v√† ti·ªÅn x·ª≠ l√Ω (1 trang)

#### 5.1. Dataset Multi30K (en-fr)
- **K√≠ch th∆∞·ªõc:**
  - Train: 29,000 c·∫∑p c√¢u
  - Validation: 1,000 c·∫∑p c√¢u
  - Test: 1,000 c·∫∑p c√¢u
- **ƒê·∫∑c ƒëi·ªÉm:** C√¢u ng·∫Øn (10-15 t·ª´), m√¥ t·∫£ h√¨nh ·∫£nh

#### 5.2. Tokenization
- S·ª≠ d·ª•ng **spaCy** (en_core_web_sm, fr_core_news_sm)
- T√°ch t·ª´, chuy·ªÉn v·ªÅ lowercase
- **V√≠ d·ª•:**
  - Input: "A man sitting on a bench."
  - Tokens: ['a', 'man', 'sitting', 'on', 'a', 'bench', '.']

#### 5.3. X√¢y d·ª±ng t·ª´ ƒëi·ªÉn (Vocabulary)
- Gi·ªõi h·∫°n: **10,000 t·ª´ ph·ªï bi·∫øn nh·∫•t** m·ªói ng√¥n ng·ªØ
- Tokens ƒë·∫∑c bi·ªát: `<unk>`, `<pad>`, `<sos>`, `<eos>`
- X·ª≠ l√Ω t·ª´ ngo√†i t·ª´ ƒëi·ªÉn (OOV) ‚Üí `<unk>`

#### 5.4. Padding & Packing
- **Padding:** ƒê·ªìng b·ªô ƒë·ªô d√†i batch ‚Üí `pad_sequence()`
- **Packing:** T·ªëi ∆∞u t√≠nh to√°n ‚Üí `pack_padded_sequence()`
- **S·∫Øp x·∫øp batch:** Theo ƒë·ªô d√†i gi·∫£m d·∫ßn (`enforce_sorted=True`)

---

### 6. X√¢y d·ª±ng m√¥ h√¨nh (2 trang)

#### 6.1. Encoder
- **Input:** Chu·ªói token ti·∫øng Anh ‚Üí Embedding (dim=256)
- **LSTM:** 2 layers, hidden_size=512, dropout=0.5
- **Output:** Context vector (h_n, c_n)

**C√¥ng th·ª©c:**
```
(h_t, c_t) = LSTM(embed(x_t), (h_{t-1}, c_{t-1}))
```

#### 6.2. Decoder
- **Input:** Token ti·∫øng Ph√°p ·ªü b∆∞·ªõc t + context vector
- **LSTM:** 2 layers, hidden_size=512, dropout=0.5
- **Output:** Ph√¢n ph·ªëi x√°c su·∫•t t·ª´ ti·∫øp theo

**C√¥ng th·ª©c:**
```
(h_t, c_t) = LSTM(embed(y_{t-1}), (h_{t-1}, c_{t-1}))
p(y_t) = softmax(Linear(h_t))
```

#### 6.3. Seq2Seq
- K·∫øt n·ªëi Encoder v√† Decoder
- **Teacher forcing ratio:** 0.5 (50% d√πng ground truth)

**B·∫£ng tham s·ªë:**
| Tham s·ªë | Gi√° tr·ªã |
|---------|---------|
| Hidden size | 512 |
| Embedding dim | 256-512 |
| S·ªë layer LSTM | 2 |
| Dropout | 0.3-0.5 |
| Teacher forcing ratio | 0.5 |

---

### 7. Hu·∫•n luy·ªán m√¥ h√¨nh (1 trang)

#### 7.1. C·∫•u h√¨nh hu·∫•n luy·ªán
- **Loss function:** CrossEntropyLoss (ignore padding)
- **Optimizer:** Adam (lr=0.001)
- **Scheduler:** ReduceLROnPlateau (patience=2)
- **Epochs:** 10-20
- **Batch size:** 32-128
- **Early stopping:** patience=3

#### 7.2. K·∫øt qu·∫£ hu·∫•n luy·ªán
**Bi·ªÉu ƒë·ªì Train/Val Loss:**
- Ch√®n h√¨nh `results/training_history.png`
- Ph√¢n t√≠ch: Model h·ªôi t·ª• sau X epochs, val loss th·∫•p nh·∫•t = Y

**Checkpoint:**
- L∆∞u best model t·∫°i `checkpoints/best_model.pth`

---

### 8. ƒê√°nh gi√° m√¥ h√¨nh (1.5 trang)

#### 8.1. BLEU Score
- **Gi·ªõi thi·ªáu:** BLEU (Bilingual Evaluation Understudy) ƒëo overlap n-gram
- **C√¥ng th·ª©c:** BLEU-n = BP √ó exp(1/n √ó Œ£ log(precision_i))
- **K·∫øt qu·∫£:**

**Bi·ªÉu ƒë·ªì BLEU Scores:**
- Ch√®n h√¨nh `results/bleu_scores.png`

| Metric | Score |
|--------|-------|
| BLEU-1 | XX.XX% |
| BLEU-2 | XX.XX% |
| BLEU-3 | XX.XX% |
| BLEU-4 | XX.XX% |

#### 8.2. V√≠ d·ª• d·ªãch
**D·ªãch t·ªët:**
```
English:    A man is sitting on a bench.
Reference:  Un homme est assis sur un banc.
Hypothesis: Un homme assis sur un banc.
BLEU:       85.2%
```

**D·ªãch k√©m:**
```
English:    The cat is sleeping on the sofa.
Reference:  Le chat dort sur le canap√©.
Hypothesis: Le chat <unk> sur le canap√©.
BLEU:       42.1%
```

---

### 9. Ph√¢n t√≠ch l·ªói (1 trang)

#### 9.1. 5 v√≠ d·ª• l·ªói d·ªãch (t·ª´ `results/error_analysis.json`)

**V√≠ d·ª• 1:**
- **Source:** ...
- **Reference:** ...
- **Hypothesis:** ...
- **BLEU:** XX%
- **L·ªói ph√°t hi·ªán:** T·ª´ v·ª±ng OOV (Out-of-Vocabulary)

**V√≠ d·ª• 2-5:** (T∆∞∆°ng t·ª±)

#### 9.2. Ph√¢n lo·∫°i l·ªói
| Lo·∫°i l·ªói | T·ª∑ l·ªá |
|----------|-------|
| T·ª´ v·ª±ng OOV (` <unk>`) | 40% |
| C√¢u qu√° ng·∫Øn - m·∫•t th√¥ng tin | 30% |
| C√¢u qu√° d√†i - th·ª´a t·ª´ | 20% |
| Thi·∫øu d·∫•u c√¢u | 10% |

---

### 10. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn (1 trang)

#### 10.1. Th√™m Attention Mechanism
- **V·∫•n ƒë·ªÅ:** Context vector c·ªë ƒë·ªãnh ‚Üí m·∫•t th√¥ng tin v·ªõi c√¢u d√†i
- **Gi·∫£i ph√°p:** Attention ƒë·ªông (Bahdanau/Luong)
- **K·ª≥ v·ªçng:** C·∫£i thi·ªán BLEU +5-10%

#### 10.2. Beam Search
- **Hi·ªán t·∫°i:** Greedy decoding (ch·ªçn token t·ªët nh·∫•t)
- **ƒê·ªÅ xu·∫•t:** Beam search (beam size = 3-5) ‚Üí nhi·ªÅu hypotheses
- **K·ª≥ v·ªçng:** BLEU +2-3%

#### 10.3. Subword Tokenization (BPE)
- **V·∫•n ƒë·ªÅ:** OOV v·ªõi t·ª´ hi·∫øm
- **Gi·∫£i ph√°p:** Byte Pair Encoding ‚Üí chia t·ª´ th√†nh subwords
- **V√≠ d·ª•:** "unbelievable" ‚Üí "un" + "believable"

#### 10.4. Dataset l·ªõn h∆°n (WMT 2014)
- Multi30K: 29,000 c√¢u
- WMT 2014: ~36 tri·ªáu c√¢u
- ‚Üí Model h·ªçc ƒë∆∞·ª£c nhi·ªÅu pattern h∆°n

#### 10.5. Kh√°c
- Layer normalization
- Scheduled sampling (gi·∫£m teacher forcing theo epoch)
- Data augmentation (back-translation)

---

### 11. K·∫øt lu·∫≠n (0.5 trang)
- **ƒê√£ l√†m ƒë∆∞·ª£c:**
  - ‚úÖ X√¢y d·ª±ng Encoder-Decoder LSTM t·ª´ ƒë·∫ßu
  - ‚úÖ X·ª≠ l√Ω d·ªØ li·ªáu Multi30K (tokenization, vocab, padding/packing)
  - ‚úÖ Hu·∫•n luy·ªán v·ªõi early stopping, checkpoint
  - ‚úÖ ƒê√°nh gi√° BLEU score
  - ‚úÖ Ph√¢n t√≠ch l·ªói d·ªãch thu·∫≠t
  - ‚úÖ H√†m `translate()` ho·∫°t ƒë·ªông v·ªõi c√¢u m·ªõi
  
- **K·∫øt qu·∫£:**
  - BLEU-4: XX.XX% tr√™n test set
  - Model parameters: ~XX tri·ªáu
  
- **H·∫°n ch·∫ø:**
  - Context vector c·ªë ƒë·ªãnh ‚Üí m·∫•t th√¥ng tin v·ªõi c√¢u d√†i
  - Dataset nh·ªè ‚Üí kh·∫£ nƒÉng t·ªïng qu√°t h√≥a h·∫°n ch·∫ø
  
- **H∆∞·ªõng ph√°t tri·ªÉn:**
  - Th√™m attention, beam search, BPE
  - So s√°nh v·ªõi Transformer

---

### 12. T√†i li·ªáu tham kh·∫£o

1. Sutskever, I., Vinyals, O., & Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. In *NIPS* (pp. 3104-3112).

2. Cho, K., Van Merri√´nboer, B., Gulcehre, C., et al. (2014). Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. *arXiv preprint arXiv:1406.1078*.

3. PyTorch Documentation: torch.nn.LSTM. https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html

4. Multi30K Dataset: https://github.com/multi30k/dataset

5. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). BLEU: a method for automatic evaluation of machine translation. In *ACL* (pp. 311-318).

---

### Ph·ª• l·ª•c
- Code ƒë·∫ßy ƒë·ªß: `main.ipynb`
- Checkpoint: `checkpoints/best_model.pth`
- K·∫øt qu·∫£: `results/*.json`, `results/*.png`

---

## Checklist n·ªôp b√†i

‚úÖ **M√£ ngu·ªìn:**
- [ ] `main.ipynb` (Jupyter Notebook) - ch·∫°y ƒë∆∞·ª£c t·ª´ ƒë·∫ßu ƒë·∫øn cu·ªëi
- [ ] C√°c file `.py` trong th∆∞ m·ª•c `src/`
- [ ] C√≥ comment r√µ r√†ng, c·∫•u tr√∫c s·∫°ch

‚úÖ **B√°o c√°o PDF:**
- [ ] ƒê·∫ßy ƒë·ªß n·ªôi dung theo outline tr√™n
- [ ] C√≥ s∆° ƒë·ªì ki·∫øn tr√∫c model
- [ ] C√≥ bi·ªÉu ƒë·ªì train/val loss
- [ ] C√≥ bi·ªÉu ƒë·ªì BLEU scores
- [ ] C√≥ 5 v√≠ d·ª• l·ªói d·ªãch + ph√¢n t√≠ch
- [ ] C√≥ tr√≠ch d·∫´n t√†i li·ªáu tham kh·∫£o

‚úÖ **Checkpoint m√¥ h√¨nh:**
- [ ] `best_model.pth` (file .pth)

‚úÖ **Ki·ªÉm tra l·∫ßn cu·ªëi:**
- [ ] Notebook ch·∫°y ƒë∆∞·ª£c tr√™n Google Colab ho·∫∑c m√°y local
- [ ] Kh√¥ng sao ch√©p code t·ª´ ngu·ªìn kh√°c
- [ ] H√†m `translate(sentence: str) -> str` ho·∫°t ƒë·ªông ƒë√∫ng
- [ ] BLEU score ƒë∆∞·ª£c t√≠nh tr√™n test set (tr√™n t·∫≠p test)
- [ ] B√°o c√°o PDF xu·∫•t ra ƒë·∫πp, kh√¥ng l·ªói font

---

**L∆∞u √Ω:**
- B√°o c√°o n√™n vi·∫øt b·∫±ng Microsoft Word ho·∫∑c LaTeX
- Font: Times New Roman, size 13 (ho·∫∑c 12)
- CƒÉn l·ªÅ: tr√°i ph·∫£i 2cm, tr√™n d∆∞·ªõi 2.5cm
- H√¨nh ·∫£nh ph·∫£i r√µ n√©t, c√≥ caption v√† s·ªë th·ª© t·ª±
- Tr√≠ch d·∫´n theo chu·∫©n (IEEE, APA, ...)

---

**SUCCESS!** üéâ
